Kubectl cheat sheet

--> Create NS

# kubectl create ns dev-ns

--> Create Service

# kubectl create service  clusterip redis-service --tcp=6379

--> Create Deployment

# kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3

--> Create POD / Expose it & Label it

# kubectl run custom-nginx --image=nginx --port=8080 -l tier=db

--> Expose existing Pod

# kubectl expose pod redis --port=6379 --name redis-service

--> Create Service

# kubectl create deployment redis-deploy -n dev-ns --image=redis --replicas=2

--> Create Pod & Service (clusterIP) in a single command

Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.

# kubectl run httpd --image=httpd:alpine --port=80 --expose

--> Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.

Step 1: Create the deployment YAML file
# kubectl create deployment redis-deploy --image redis --namespace=dev-ns --dry-run=client -o yaml > deploy.yaml
Step 2: Edit the YAML file and add update the replicas to 2
Step 3: Run kubectl apply -f deploy.yaml to create the deployment in the dev-ns namespace.

You can also use kubectl scale deployment or kubectl edit deployment to change the number of replicas once the object has been created.


--> Selector with multiple Labels
# kubectl get pods --selector=env=prod,bu=finance,tier=frontend

--> Taint Node
--> Create a taint on node01 with key of 'spray', value of 'mortein' and effect of 'NoSchedule'

Syntax
# kubectl taint nodes node1 key1=value1:NoSchedule

Answer
# kubectl taint node node01 spray=mortein:NoSchedule

--> Add Tolerataion to Pod
    - Question :- Create another pod named 'bee' with the NGINX image, which has a toleration set to the taint Mortein
        Image name: nginx
        Key: spray
        Value: mortein
        Effect: NoSchedule
        Status: Running

    - Answer :-
        - Create pod yaml with Toleration
        - bee.yaml
            apiVersion: v1
            kind: Pod
            metadata:
            name: bee
            spec:
            containers:
            - name: nginx
                image: nginx
                imagePullPolicy: IfNotPresent
        ----> tolerations: <----
            - key: "spray"
                value: "mortein"
                effect: "NoSchedule"

--> Remove taint from node

Syntax 
# kubectl taint nodes <nodename> <taint>-
    # taint value = Copy exact taint line from #kubectl describe node taint value
    # add - (dash) at the end to remove

# kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

--> Add Label - Some examples are

# kubectl label pods my-pod new-label=awesome 

# kubectl label node node01 color=blue

--> Dry Run and create only yaml through kubectl command example (Note --dry-run=client and -o yaml)

# kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 >> busybox.yaml

--> To view the kubelet log

# journalctl -u kubelet -l

--> Check static pod on local node

    # docker ps

--> List all the events in current namespace

    # kubectl get events

--> To inspect metrics
    # kubectl top node/nodes

    # kubectl top pods

--> To see logs of a pod/application

    # kubectl logs <pod name>

    # kubectl logs <pod name> <container name>

--> To set image on existing deployment

    # kubectl set image deployment <deployment name> <container name>=<image name>

--> To see rollout status / history

    # kubectl rollout status deployment <deployment>

    # kubectl rollout status deployment <deployment>

--> Create configmap

    # kubectl create configmap <configmap name> --fron-literal=<key>=<value>

    # Example :-

        # kubectl create configmap colourconfigmap --fron-literal=APP_COLOR=blue

--> Create Generic/Custom Secret (without username/password as a key)

    # ktl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

--> To login to container

    # ktl exec -it <pod name> -n <namespace> -- sh/bash

--> Volumes are volumes to be shared

--> VolumeMounts are to mount above volume to pod

--> To check the state of the initContainer
    # kubectl describe pod 
        # check for "State" of that "initContainer"


--> in Yaml Try to add command in container like this (Try to avoid in single line)

    command:
      - sh
      - -c
      - sleep 20

---> Cordon-Uncordon / Drain nodes
    
        - #kubectl drain node-1 (Move all pods running on that node and make that node unschedulable)
            - Drain = Cordon + Unschedulable
        - #kubectl cordon node-1 (Just mark node unschedulable, it doesn't move pods to another node)
        - #kubectl uncordon node-1 (Aftre reboot, To make the node schedulable)

--> Upgrade Cluster

    --> If there are only 2 nodes (master/worker), upgrade only one at a time

    --> Exact upgrade commands

        --> Master

            # kubeadm version / kubectl version --short / kubectl get nodes
            # kubeadm upgrade plan
            # apt-mark unhold kubeadm
            # apt-get update && apt-get install -y kubeadm=1.19.0-00
            # kubeadm upgrade plan
            # kubeadm upgrade apply v1.19.0
            # apt-get update && apt-get install -y kubelet=1.19.0-00
            # kubectl uncordon controlplane

        --> To upgrade worker node

            # Drain node from master 
                # kubectl drain node01
            # On worker node
                # apt-get update && apt-get install -y kubeadm=1.19.0-00
                # kubeadm upgrade node
                # apt-get update && apt-get install -y kubelet=1.19.0-00
                # systemctl restart kubelet
        
        # Uncordon from master

            # kubelet uncordon node01

        --> Final Upgrade plan ( Never use apt-get upgrade - always use update )
            - If only 2 node cluster (one master/one worker) then check taint on master, to know it can accept pod or not.

            - On Master

                - Drain master node
                - update kubeadm on master using apt
                - kubeadm upgrade plan
                - kubeadm upgrade apply <version>
                - update kubelet
                - systemctl restart kubelet
                - kubeadm uncordon masternode

            - On slave

                - Drain node from master
                - update kubeadm using apt
                - kubeadm upgrade node
                - update kubelet
                - systemctl restart kubelet
                - kubeadm uncordon node ( from master)

            - Verify status of nodes

                - kubectl get nodes

--> Decode Certificate 
    # openss x509 -in <certificate> -text -noout

--> Check docker 
    # docker ps & get container id
    # docker logs <container id> # to check logs of that container
    # docker logs <container id> -f # to check logs on the go

--> Certificate API
    - All the certificate related operations are carried out by Controller-Manager
    - To encode certificate and make it inline
        # cat certificate.csr |base64 -w0
    - To decode the certificate data
        # echo "encodedcontent" |base64 --decode
    
    - See the csr
        # kubectl get csr
        # kubectl certificate approve <name of certificate/csr>

        # To inspect the csr
            # kubectl describe <csr name>

        # To Deny csr
            # kubectl certificate deny <csr name>

        # To Delete CSR
            # kubectl delete csr <csr name>

--> Kubconfig ( Configuration file to save in home directory to switch context )
    - Very Important

    - clusters
    - contexts
    - users

    - To view current config
        # kubectl config view
        # kubectl config view --kubeconfig=<file name>

    - To change context
        # kubectl config get-contexts
        # kubectl config use-context <context name>
        # kubectl config use-context dev-user@test-cluster-1 --kubeconfig=my-kube-config
        # If you want to use ~/.kube/config file ( don't give argument --kubeconfig)

            # kubectl config use-context <context name>

    - The permission on ~/.kube/config file must be "600" (rw - only by owner)

--> If you are exporting using -o yaml and get multiple metadata content, exclude using grep -v like

    # kubectl get pod nginx -o yaml | grep -v "f:" > pod.yaml

--> Bash profile kubectl autocomplete (from kubernetes cheat sheet)

--> Practice dry-run most

--> 

# Create a pod with nginx image
    $ kubectl run nginx --image=nginx

# Create the pod template file with dry-run
    $ kubectl run nginx --image=nginx --dry-run=client -o yaml>nginx.yaml

# Create a deployment
        $ kubectl create deploy nginx-app --image=nginx

# Scale a deployment
    $ kubectl scale deploy nginx-app --replicas=3

# Create nginx-service and expose
    $ k expose pod nginx--name=nginx-service --port=80

# Create a secret name my-secret with username shekhar

    $ kubectl create secret generic my-secret --from-literal username=shekharâ€‹

--> Understand nodeport - port - mapping

--> Authorization

    - Process
        - Create Role object
        - Create role binding object
    - To view RBAC
        # kubectl get rbac
        # kubectl get rolebindings
    - To view role (developer)
        # kubectl describe role <Role Name>
        # kubectl describe role developer
    - To view rolebindigs
        # kubectl describe rolebindings <rolebinding name>
    - To check my access
        # kubectl auth can-i <permission>
            Eg. - # kubectl auth cani-i create deployments
                  # yes
                  # kubectl auth cani-i delete deployments
                  # no
    - To check access of a user as an admin
        # kubectl auth can-i create deployment --as <username>
            Eg. # kubectl auth can-i create deployment --as otheruser
                # yes
                # kubectl auth can-i delete deployment --as otheruser
                # no
        or
            # kubectl <command> --as <username>
            # kubectl get pods --as dev-user

        - We can specify different namespace in this command
            # kubectl auth can-i create deployment --as developer --namespace dev
            # yes
            # kubectl auth can-i create deployment --as developer --namespace prod
            # no
    - We can add "ResourceName" in manifest file, to provide more specific access
        - Like access on Pod
            - Only Blue/Red Pod
    
- To check authorization mode
    - Check kube-api pod configuration
        # kubectl describe pod kube-apiserver -n kube-system |grep -i authorization

--> Example Commands to create role and role bindings
    - These role/rolebindings are namespaced ( They are created within namespace )
        - If we don't sppecify --namespace argument, it will be created in/for default namespace
    - ApiGroup is different for each resource like deployment/pod/volume etc, 
        so use specific ApiGroup for each resource them while creating role.

    - kubectl create role --help
    - kubectl create role developer --verb=create,get,delete --resource=pods --namespace=default
    - kubectl create rolebinding --help
    - kubectl create rolebinding dev-user-binding --role
    - kubectl edit role
    - kubectl edit rolebindings

--> There are 2 types of resources
    - namespaced
    - Cluster
    - To list the namespaced & non-namespaced resources
        # kubectl api-resources --namespaced=true
        or
        # kubectl api-resources --namespaced=false

--> Clusterroles & Clusterrolebindings
    - They are same as role & rolebindings but they are for cluster resources (clusterwide)

    - If you create a cluster role and map it to a user, that user gets that access across the cluster in any namespace

    - Examples

        - To give all node access to user michelle
            - kubectl create clusterrole mnoderole  --verb=* --resource=node
            - kubectl create clusterrolebinding mnoderole --clusterrole=mnoderole --user=michelle

        - To give storage management access to michelle
            - kubectl create clusterrole storage-admin --verb=* --resource=persistentvolumes,storageclasses
            - kubectl create clusterrolebinding michelle-storage-admin --clusterrole=storage-admin --user=michelle

--> Configure Credentials for private registry

    - kubectl create secret docker-registry regcred

        Eg. # kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com

    - And use those credentials in pod spec at imagePullSecrets

--> Security Context
    - Like runAsUser:
    - Capabilities

    Note:
    - Security context can be added on pod and container both but Capabilities can be added to container only not on pod.
    - Check for another securityContext in yaml, if the one you added doesn't work.
    

--> To check which user used command inside container
    # kubectl exec -it <pod name> whoami

--> To run a command in a container
    # kubectl exec --it <pod name> <command>

    # root user id is "0", So if you want to run anything as root in container/pod use
        securityContext:
          runAsUser: 0

Very Important Note 

    - check for TAB if you get error "error converting YAML to JSON: yaml: line 47: found character that cannot start any token"
    - Check and verify Indentation from doc if you get error

--> Netowrk Policy

    - Create Network policy to allow Ingress traffic from pod-name & Port
        - Create netowrk policy
        - Add Ingress policy type
        - PodSelector to apply policy to that pod

    - Network policies aren't supported on Flannel


--> Volume / Storage

    - To mount host volume inside pod
        - Create Volume at spec level
        - Create volumemount inside container
        - Use "hostPath" in volume ( Not "local")

    - Create PV
    - Create PVClaim
    - Use PVClaim inside POD
    - Check Reclaim

    - Check for accessmode if the PVClaim not showing available (May be size mismatch)

    - Inside Pod

        - PVClaim will be there as Volume
        - Mount inside container as a volume
        - ReClaim Policy
            - Retain: When the PVC is deleted, the PV still exists. The volume is considered released, but it is not yet available because the previous data remains on the volume. If you want to delete it, you must do manually.
                - PV will not be deleted nor available

            - Delete: when the PVC is deleted, the PV and the associated storage in the external infrastructure (i.e. the Cinder storage in our case) are both deleted.
        
        - If PVC is being used in any pod, it can't be deleted.
        
        - PV status will show "Bound" if its bounded to PVC and it will show "Released" Once pvc is deleted.

    - If you use Storage Class, don't need to create PV, just create SC on cluster & PersistentVolumeClaim inside pod.

    - Understand available PV / PVC sizes
    
    - https://kubernetes.io/docs/concepts/storage/storage-classes/

    - kubectl get storageclasses/sc
        - Different Volume Binding modes
            - Immediate (default)
            - WaitForFirstConsumer

    - Notes while creating PVC
        - Access mode must match with PV
        - Storageclass / 

--> Ingress

    - In exam there will be questions to create Ingress rules

        # Ingress Rules/Resources without host
        - https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource
        
        # Ingress rule with Host
        - https://kubernetes.io/docs/concepts/services-networking/ingress/#hostname-wildcards


    - To see deployed ingress controller
        - Check pods deployed in kube-system ns
        # kubectl get pods --all-namespaces/-n kube-system

    - Ingress Controller Deployment
        # ktl create ns ingress-space
        # kn ingress-space
        # ktl create configmap nginx-configuration
        # ktl create serviceaccount ingress-serviceaccount
        # Create role & Rolebindings
            - Role 
                - Create/Get configmaps
                - Get endpoints, namespaces, pods, secrets
            - RoleBinding
                - Serviceaccount
                - New role
        # Create ingress deployment
        # Create ingress service to expose above deployment
        # Create Ingress resources in application namespace
        
        # Ingress Resource (Rules) must be deployed in the namespace where actual services are deployed, not in ingress namespace

--> Install cluster using kubeadm

    - Installation steps :-

        - Disable Swap on all nodes
            $ vi /etc/fstab
                - Comment swap line
                - Reboot
            - sudo swapoff -a
        - check br_netfiler module loaded or not, if not, load module
            $ lsmod | grep br_netfilter
            $  sudo modprobe br_netfilter
        - Follow docker installation document
        - Install kubeadm
        - We don't need to configure group driver in docker
        - Follow "Using kubeadm to Create a Cluster" document
            - Initialize control plane node using pod network cidr & --apiserver-advertise-address arguments
        - Install Pod Network Addon
        - Join worker nodes
        - Run nginx and check

--> Troubleshooting

    - To Check logs
        # kubectl logs <pod name> -f

    - To check control plane components
        - To check log of previous pod (If cluster deployed by kubeadm)
            # kubectl logs <pod name> -f --previous

            - Verify all pods are running in kube-system namespace
            - Check logs 
                # kubectl logs kube-apiserver

    - If control plane components are deployed as services (not by kubeadm)
        - Check control plane services
            # service kube-apiserver status
            # service kube-controller-manager status
            # service kube-scheduler status
        - To check logs
            # journalctl -u kube-apiserver etc.

    - Cluster Troubleshooting
        - Cluster information can be checked
            # kubectl cluster-info
        - Check certificate path / mounted volume / volume names
        - Control plane manifest files should be there in /etc/kubernetes/manifests

    - Worker node Troubleshooting

        - Check kubelet & kube-proxy services on worker nodes

        - kubectl get nodes
        - kubectl describe node <nodename>
            - If node is out of disk - outofdisk flag it set to true
            - Same for other
            - When node is ready "Ready" flag is true, which is not an error.

        - When worker node stop communicating to master, above stats show "Unknown"
            - Check "LastHeartbeatTime" the time when node crashed.

        - Check worker node services kubelet/kube-proxy
            - cpu / memory / disk space on node
            - service kubelet status / systemctl status kubelet / 
                For additional log add "-l" like # systemctl status kubelet -l
            - journalctl -u kubelet

        - Check kubelet certificate
            - /var/lib/kubelet/worker.crt

        - Check kubelet process configuration file with

            # ps -ef |grep -i kubelet

        - Kubelet config file
            - When we check $ service kubelet status
                - We can see file path at "Drop-In"
                    - /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            /var/lib/kubelet/config.yaml

        - Check kubelet config file & server port, which is api server port / Default - 6443

        - If you are updating config file, reload daemon and then restart service
            # systemctl daemon-reload
            # systemctl restart kubelet

        - Cluster information can be checked
            # kubectl cluster-info

    - Network
        - Check if network addon is installed or not
        - Check controller component description / logs 
        - Check configmaps

        - Kubeproxy mounts config from configmap, so we have to specify exact filename which inside configmap.
        - Check for volume and mounts for pod env & configurations

        - Check DNS resolution
            - If not work
                - Check kube-dns service in kube-system ns
                - Match its selector with core-dns pods